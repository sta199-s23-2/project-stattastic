---
title: "Just Dance(ability)"
subtitle: "Report"
format: html
editor: visual
execute:
  echo: false
---

## Introduction and data

Within the music industry there is always a desire to be able to predict the next big song. Different factors will always play into whether or not a song becomes popular. We wish to explore how danceability, in particular, affects the popularity of songs. To dig a bit deeper, we want to see if the impact of danceability on popularity changes depending on which genre we are exploring. We are hoping that by exploring this correlation we can create a predictive model for popularity based on danceability. Based on the correlations we find by genre this model may be more accurate for certain genres more so than others. We have three research questions: How does danceability affect the popularity of songs? How does this vary between genres? How do other song variables influence this relationship.

Our research project will center around the Spotifyr, an open source music dataset provided by Spotify. The dataset contains around 5,000 songs from six major music categories (EDM, Latin, Pop, R&B, Rap, and Rock). These genres were selected from Every Noise, which is a visualization created and maintained by a genre taxonomist. The top four genre's from Every Noise were used to find 20 playlists that contained relevant songs. The 5000 songs from these playlists were added to the dataset that we are analyzing. This data was gathered for a tidytuesday podcast. For each song, the data set contains general information such as the songs album, artist, release data, but also more interesting statistics on danceability, energy, and popularity. The Spotify Web API was used to gain information about the songs. Spotify has twelve different features that are associated with songs. These are included within the dataset as variables.

We considered different ethical questions surrounding data collection. Specifically, we explored how songs were chosen from Spotify and acquisition bias; however, do not believe there are significant ethical concerns related to this data and data collection.

We hypothesize that there will be a positive correlation between danceability and popularity, but that correlation will differ between genres. We expect the Latin genre to have the greatest correlation. Additionally, we hypothesize that the tempo and the energy of the song (when taken into consideration with the danceability rating) will better be able to predict track popularity rating.

## Literature Review

Our data set was originally found through the #tidytuesday project on GitHub. Kaylin Pavlik, a blogger, explored this dataset in various ways. She explored audio features by genre. She found that EDM tracks are the least likely to have acoustic features, but highly likely to have high energy with low valence. Additionally, through density/line plots, she found that danceability, tempo, valence, and energy had the largest differences by genre. We are building on her research by specifically analyzing the differences between danceability by genre. Our research question digs deeper into this question. Additionally, she found that Latin music was very danceable, while rock music had very little danceability. We will continue to look into the specifics of these differences through linear regression models \^\[https://www.kaylinpavlik.com/classifying-songs-genres\].

Amri Rohman also analyzed this dataset \^\[https://rpubs.com/amrirohman/spotify-song\]. She specifically looked at songs and their popularity. In 2020 she found that the most popular Spotify song was Dance Monkey. She also looked at the differences in tempo for each genre, faceted by different eras of music. She found that rap songs in the 2000s had slower tempos. We are going to build on these research questions by looking at the popularity of songs by genre faceted by dancability. We are both analyzing how different song features vary across genres.

Although other groups/people have looked into the same Spotifyr datset from tidytuesday, we are planning on looking into vastly different research questions. Because this is a large data set with many variables, there are many questions to look into. We hope to dive into popularity to analyze reasons as to why songs and genres have gained notoriety.

## Methodology

We plan to answer this research question through a linear regression analysis looking at if the danceability of a song can be predictive for its popularity. This regression analysis will likely consider the energy and tempo variables in the regression analysis. This analysis will be faceted across genres as well. First, we will look to see at the general relationship between danceability rating and track popularity rating of songs in the data. Then, we will look to into the level of this relationship in terms of different types of music by assessing genre as variable. In addition to these basic visualizations, we will also calculate a number of summary statistics that will further assist in other analyses and allow us to better understand the data. This includes calculation of the mean danceability rating and popularity ratings specifically to each genre.

Additionally, we will conduct a regression analysis to predict popularity on the basis of danceability ratings. This linear regression will have additional models looking specifically at Latin and Rap genres as well. There will also be a regression analysis conducted to assess how other variables related to a song influence the relationship between danceability and popularity. The approriate type of regression for this would be a multiple linear regression, taking into consideration multiple explanatory variables to predict track popularity. The specific quantitative variables we want to consider are energy and tempo because of their relation to danceability in this regression analysis as similarly considered by a similar project by Ochi et al. (n.d.). We will assess these types of regressions with both additive and interaction models to see which type of model best fits the data.

We will conduct calculations for adjusted R-squared for the linear regression and AIC for both types of regression models, in addition to mean squared error for the prediction models to confirm they are well fit. We used adjusted R-squared rather than r-squared because it takes into account the amount of variables that are being used. We were analyzing different variables in this dataset, so adjusted r-square was a better metric to use. These two metrics were used to determine the best fit model, specifically when using the many variables that we were assessing. To further assess these models created, we will split the data into training and testing data. All the models would be fitted to training data and then tested to predict popularity for on the testing data. We will calculate mean squared error for the prediction for the training and testing to determine if our models are fit at predicting for popularity on the respective explanatory variables. This MSE calculation would ensure that the results on model fitness to ensure that our model is not overfit. This will allow for adequate conclusions to be drawn from our model and subsequent usability of our models for predicting popularity of Spotify songs.

## Data

```{r}
#| label: load-libraries
#| message: false
#| warning: false

library(tidyverse)
library(tidymodels)
library(kableExtra)
library(knitr)
```

```{r}
#| label: load-data
#| message: false
#| warning: false

spotify_songs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')

head(spotify_songs)

nrow(spotify_songs)
```

## Results and Analysis

First we will plot out general popularity of songs versus their adaptability score. This will allow us to visualize how dance-ability affects song's popularity overall in the data set.

```{r}
#| label: popularity-vs-danceability-general
set.seed(33)

spotify_songs |>
  sample_n(size = 1500) |>
  ggplot(
    aes(
      x = danceability,
      y = track_popularity,
      color = playlist_genre
    )
  ) +
  geom_point(color = "steelblue", alpha = .5, size = .5) +
  geom_smooth(method = "lm", color = "black") +
  labs(
    x = "Danceability Rating",
    y = "Track Popularity Rating",
    title = "How does danceability affect the popularity of a song?"
  )
```

From the above plot we see a very subtle correlation between the danceability rating and its popularity for this random sampling of 1000 songs. However, with so many data points on this plot danceability likely is not a great predictor for track popularity **generally**. There is no significant linear relationship between danceability and popularity.

Next, we'd like to see what this plot looks like if we facet it by the genre.

```{r}
#| label: popularity-vs-danceability-genre
set.seed(33)

spotify_songs |> 
  sample_n(size = 1500) |>
  ggplot(
    aes(
      x = danceability,
      y = track_popularity,
      color = playlist_genre
    )
  ) +
  geom_point(size = .3) +
  geom_smooth(method = "lm", color = "black") +
  facet_wrap(~playlist_genre) +
  labs(
    x = "Danceability Rating",
    y = "Track Popularity Rating",
    title = "Danceability vs popularity of songs in different genres", 
    color = "Genre"
  )
```

Here we can see that there is certainly a difference in how dance-ability affects the popularity of a track depending on the songs genre. Our highest correlations found here are rap and pop while we even see a slightly decreasing slope for r&b. However, its worth noting that some genres tend to have a higher dance-ability score on average such as Latin and rap. While rock has a higher correlation than some other genres, on average its songs are less dance-able than the others.

### **Linear Regression**

Now, we'd like to train a linear regression model to predict popularity using dance-ability. We'll train three separate models here before testing them on the testing data.

1.  A general model that can be used for any genre.

2.  A model that predicts how dance-ability affects the popularity of a song from the Latin genre.

3.  A model that predicts how dance-ability affects the popularity of a song from the rap genre.

    ```{r}
    #| label: train-test-data

    set.seed(33)

    popularity_split <- initial_split(spotify_songs, prop = 0.80, strata = "playlist_genre")

    train_data <- training(popularity_split)
    test_data <- testing(popularity_split)

    glimpse(test_data)
    ```

```{r}
#| label: linear-regression-models

general_dance <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability, data = train_data) 

latin_songs <- 
  train_data |>
  filter(playlist_genre == "latin")

latin_dance <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability, data = latin_songs)

rap_songs <- 
  train_data |>
  filter(playlist_genre == "rap")

rap_dance <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability, data = rap_songs)

general_dance |>
  tidy()

latin_dance |>
  tidy()

rap_dance |>
  tidy()
```

$pop_{general} = 35.3 + 10.9 * danceability$

For a 1 increase in danceability rating, there's an estimated mean change of a song's popularity rating (general song w/ no designated genre) by 11.1.

When danceability is 0, there is an estimated mean popularity rating of 35.2.

$pop_{latin} = 41.3 + 7.94 * danceability$

For a 1 increase in danceability rating, there's an estimated mean change of a latin song's popularity rating by up by 8.8.

When danceability is 0, there is an estimated mean popularity rating of 40.7.

$pop_{rap} = 35.3 + 10.9 * danceability$

For a 1 increase in danceability rating, there's an estimated mean change of a song's popularity rating by 11.2.

When danceability is 0, there is an estimated mean popularity rating of 25.5.

**Training Data Set**

Calculations of mean squared error for the training data set from predictions.

```{r}
#| label: prediction
#General
pop_pred_general <- predict(general_dance, train_data) |>
  pull() 

mean((pop_pred_general - train_data$track_popularity)^2)
#Latin
pop_pred_latin <- predict(latin_dance, latin_songs) |>
  pull() 

mean((pop_pred_latin - latin_songs$track_popularity)^2)
#Rap
pop_pred_rap <- predict(rap_dance, rap_songs) |>
  pull() 

mean((pop_pred_rap - rap_songs$track_popularity)^2)
```

**Testing Data Set**

Calculations of mean squared error for the training data set from predictions.

```{r}
#| label: test_Dataset

latin_songs_test <- 
  test_data |>
  filter(playlist_genre == "latin")

rap_songs_test <- 
  test_data |>
  filter(playlist_genre == "rap")

#General
pop_pred_general_test <- predict(general_dance, test_data) |>
  pull() 

mean((pop_pred_general - test_data$track_popularity)^2)

#Latin
pop_pred_latin_test <- predict(latin_dance, latin_songs_test) |>
  pull() 

mean((pop_pred_latin_test - latin_songs_test$track_popularity)^2)

#Rap
pop_pred_rap_test <- predict(rap_dance, rap_songs_test) |>
  pull() 

mean((pop_pred_rap_test - rap_songs_test$track_popularity)^2)
```

**MSE Table**

Summary of the mean squared error results from both training and testing data models.

```{r}
MSE_table <- data.frame(
  general_dance_train = 623.7248, 
  general_dance_test = 618.2059,
  latin_dance_train = 647.9394, 
  latin_dance_test = 634.6527,
  rap_dance_train = 539.8089, 
  rap_dance_test = 498.709
)

MSE_table |>
  pivot_longer(cols = names(MSE_table), names_to = "Linear Regression Models", 
               values_to = "MSE" ) |>
  knitr::kable()
```

**Linear Regression Model Fit Assessments**

Now we'd like to assess how well the regression model explains the observed data. We will do this by calculating adjusted R squared values.

```{r}
#| label: adjusted-R-squared

general_dance |>
  glance() |>
  pull(adj.r.squared) 

latin_dance |>
  glance() |>
  pull(adj.r.squared) 

rap_dance |>
  glance() |>
  pull(adj.r.squared) 
```

```{r}
adjusted_R2_table <- data.frame(
  R2_general_dance = 0.003989811, 
  R2_latin_dance = 0.00103406, 
  R2_rap_dance = 0.01922207
)

adjusted_R2_table |>
  pivot_longer(cols = names(adjusted_R2_table), names_to = "Models", 
               values_to = "R Squared" ) |>
  knitr::kable()
```

Based on the adjusted R squared values, danceability rating as an explanatory variable is not a good predictor of popularity rating in general and in the latin and rap genres. The adjusted R squared values between 0.10-4.0% reveals a very low variability in popularity is explained by the regression model. It's important to note that both the rap and dance models likely have fewer samples total than the general genre model and these can impact the adjusted r-squared results.

Now we'd like to assess how well the regression model explains the observed data using AIC values.

```{r}
# | label: AIC-1

general_dance |>
  glance() |>
  pull(AIC) |>
  tibble()

latin_dance |>
  glance() |>
  pull(AIC) |>
  tibble()

rap_dance |>
  glance() |>
  pull(AIC) |>
  tibble()
```

```{r}
LR_AIC_table <- data.frame(
  AIC_general_dance = 243558, 
  AIC_latin_dance = 38407,
  AIC_rap_dance = 41963
)

LR_AIC_table |>
  pivot_longer(cols = names(LR_AIC_table), names_to = "Models", values_to = "AIC" ) |>
  knitr::kable()
```

Based on AIC calculations, the latin dance model was best fit because it had the lowest AIC value at 38407, where the rap model had the second lowest AIC of 41963, and the general model had the highest AIC of 243558. AIC selects a model with the least mean squared error, so a lower value is preferable \[http://www.sthda.com/english/articles/38-regression-model-validation/158-regression-model-accuracy-metrics-r-square-aic-bic-cp-and-more/\]. It's important to note that both the rap and dance models likely have fewer samples total than the general genre model and these can impact the AIC results.

This results are important given the MSE results showing that all the models were fit for both training and testing data. There were no major differences between the MSE for the models on the training and testing data. This shows that our models were not overfit - the models and their explanatory variables worked well at predicting popularity for both the trained data and on new data (testing data).

**Multiple Linear Regression Models and Model Fit Assessments**

Now, we'd like to train multiple linear regression models to predict popularity using dance-ability, tempo, and energy. We'll train multiple separate models.

1.  An additive model for danceability rating + tempo.
2.  An additive model for danceability rating + energy.
3.  An additive model for danceability rating + tempo + energy.
4.  An interaction model for danceability rating \* tempo.
5.  An interaction model for danceability rating \* energy.
6.  An interaction model for danceability rating \* tempo \* energy.

```{r}
#| label: multiple-linear-regression-train

#additive models

dance_plus_tempo <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability + tempo, data = train_data) 

dance_plus_tempo |>
  glance() |>
  pull(AIC)

dance_plus_energy <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability + energy, data = train_data) 

dance_plus_energy |>
  glance() |>
  pull(AIC)

dance_plus_tempo_energy <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability + tempo + energy, data = train_data) 

dance_plus_tempo_energy |>
  glance() |>
  pull(AIC) 

#interactions

dance_int_tempo <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability * tempo, data = train_data) 
dance_int_tempo |>
  glance() |>
  pull(AIC)

dance_int_energy <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability * energy, data = train_data) 

dance_int_energy |>
  glance() |>
  pull(AIC)

dance_int_tempo_energy <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability * tempo * energy, data = train_data) 

dance_int_tempo_energy |>
  glance() |>
  pull(AIC) 
```

```{r}
#| label: multiple-linear-regression-test

#additive models

dance_plus_tempo_test <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability + tempo, data = test_data) 

dance_plus_tempo_test |>
  glance() |>
  pull(AIC)

dance_plus_energy_test <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability + energy, data = test_data) 

dance_plus_energy_test |>
  glance() |>
  pull(AIC)

dance_plus_tempo_energy_test <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability + tempo + energy, data = test_data) 

dance_plus_tempo_energy_test |>
  glance() |>
  pull(AIC) 

#interactions

dance_int_tempo_test <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability * tempo, data = test_data) 

dance_int_tempo_test |>
  glance() |>
  pull(AIC)

dance_int_energy_test <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability * energy, data = test_data) 

dance_int_energy_test |>
  glance() |>
  pull(AIC)

dance_int_tempo_energy_test <- linear_reg() |>
  set_engine("lm") |>
  fit(track_popularity ~ danceability * tempo * energy, data = test_data) 

dance_int_tempo_energy_test |>
  glance() |>
  pull(AIC) 
```

```{r}
#| label: AIC-table

MLR_AIC_table <- data.frame(
  AIC_dance_plus_tempo_train = 243559.2,
  AIC_dance_plus_tempo_test = 60820.16,
  AIC_dance_plus_energy_train = 243268,
  AIC_dance_plus_energy_test =  60754.63,
  AIC_dance_plus_tempo_energy_train = 243258.8,
  AIC_dance_plus_tempo_energy_test = 60752.95,
  AIC_dance_int_tempo_train = 243561.1,
  AIC_dance_int_tempo_test =  60822.01,
  AIC_dance_int_energy_train = 243268.6,
  AIC_dance_int_energy_test = 60748.93,
  AIC_dance_int_tempo_energy_train = 243257.6,
  AIC_dance_int_tempo_energy_test = 60744.13
  ) 

MLR_AIC_table |>
  pivot_longer(cols = names(MLR_AIC_table), names_to = "Models", values_to = "AIC" ) |>
  knitr::kable()
```

Given that the interaction model between danceability rating, tempo, and energy resulted in the lowest AIC value (testing data = 60744.13; training data = 243257.6), it is the best fit of the ones assessed at modeling popularity of a song. However, there are very small differences for the AIC results across all the models. We excluded genre in the multiple-linear regression models, because we learned that genre did not have significant impacts on our previous predictive models. This caused us to shift our focus to other variables that could potentially impact our predicative models.

```{r}
#| label: MSE-calcs

mult_lin_test_6 <- predict(dance_int_tempo_energy_test, test_data) |>
  pull() 

mean((mult_lin_test_6 - test_data$track_popularity)^2)

mult_lin_train_6 <- predict(dance_int_tempo_energy, train_data) |>
  pull() 

mean((mult_lin_test_6 - train_data$track_popularity)^2)
```

```{r}
#| label: MSE-table-MLR

MSE_table <- data.frame(general_dance_train = 623.7248, 
  dance_int_tempo_energy_train = 636.1877,
  dance_int_tempo_energy_test = 604.9009)

MSE_table |>
  pivot_longer(cols = names(MSE_table), names_to = "Interaction Model", 
               values_to = "MSE" ) |>
  knitr::kable()

```

This results are important given the MSE results showing that all the models were fit for both training and testing data. Given that the interaction model between danceability, tempo, and energy had the lowest AIC value, this was the model chosen to conduct predictions from the training-testing data with MSE calculations. There were no major difference between the MSE for the models on the training and testing data. This shows that our were not overfit - the models and their explanatory variables worked well at predicting popularity for both the trained data and on new data (testing data).

## Overall Results + Next Steps + Limitations (DISCUSSION)

1.  Danceability is not a great predictor for popularity when **not** faceted by genre.

2.  Danceability has different impacts on popularity when faceted by genre.

3.  Danceability has a positive and significant effect on popularity ratings of general, latin, and rap songs. This relationship varies

    1.  Rap has the highest coefficient

    2.  General has the lowest

4.  General model had best fit according to AIC criteria (lowest mean squared error)

5.  The interactive model for danceability \* tempo \* energy has lowest AIC

6.  The additive and interactive models that included the energy variable had lower AIC values
